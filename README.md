# ğŸ’¬ Self-Healing Sentiment Classifier (CLI App)

A robust, modular CLI-based sentiment classification system that automatically handles low-confidence predictions using fallback logic â€” powered by a fine-tuned BERT model, LangGraph, and a backup zero-shot classifier.

---

## ğŸ“Œ Features

- ğŸ” **Sentiment Analysis** (Positive/Negative) using a fine-tuned `DistilBERT` model on the IMDb dataset.
- ğŸ§  **Self-Healing Logic**: If model confidence is low, fallback options kick in:
  - âœ… Backup zero-shot model (`facebook/bart-large-mnli`)
  - ğŸ‘¤ Manual user clarification (optional version)
- ğŸ“ˆ **Confidence Visualization**: See how confident the model is across inputs.
- ğŸ“Š **Fallback Statistics**: Live histogram showing how often fallback was needed.
- ğŸ’¾ **Logging**: All interactions (input, prediction, confidence, correction) are saved.

---

## ğŸš€ How It Works

The application is built as a **DAG (Directed Acyclic Graph)** using `LangGraph`, with the following pipeline:

User Input â¡ï¸ Inference â¡ï¸ Confidence Check
â¬‡ï¸
[Low Confidence Detected]
â¬‡ï¸
Backup Model Prediction
â¬‡ï¸
Final Output & Logging


---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/poison01022/self-healing-sentiment-cli.git
cd self-healing-sentiment-cli
```

### 2. Create a virtual environment (optional but recommended)

```bash
python -m venv env
env\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### ğŸ“¥ Model Download Instructions

The sentiment classifier expects a fine-tuned **DistilBERT** model saved at `./fine_tuned_model/`.

You can download it directly from the terminal using the Hugging Face Transformers library:

#### Step 1: (Optional) Authenticate with Hugging Face if using a private repo
```bash
transformers-cli login
```

### Step 2: Download and save the model locally

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "YOUR_HF_USERNAME/fine-tuned-distilbert-imdb"    #ğŸ” Replace YOUR_HF_USERNAME/fine-tuned-distilbert-imdb with the actual model path from Hugging Face Hub.

AutoTokenizer.from_pretrained(model_name).save_pretrained("fine_tuned_model")
AutoModelForSequenceClassification.from_pretrained(model_name).save_pretrained("fine_tuned_model")
```

### Step 3: (Optional) Pre-download the Backup Model

The fallback logic uses Hugging Face's facebook/bart-large-mnli model for zero-shot classification.

You can pre-download it to avoid delays during runtime:

```python
from transformers import pipeline

pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
```

### ğŸš€ Run the CLI App

Before launching the CLI, make sure the fine-tuned model is available.  
If you haven't trained it yet, run:

```bash
python fine_tune_sentiments.py
```
This will fine-tune DistilBERT on the IMDb dataset and save the model to the fine_tuned_model/ directory.

Then, start the sentiment classification CLI application:

```bash
python cli_app.py
```

The CLI supports intelligent fallback using a zero-shot model and logs all predictions with confidence scores.



ğŸ› ï¸ Commands during execution:

exit â†’ Quit the app

stats â†’ Show fallback usage statistics

plot â†’ Display confidence curve over recent inputs

### OUTPUT SAMPLE

![Image](https://github.com/user-attachments/assets/9e3f3848-fefb-4fb6-8d0e-768008478140)
![Image](https://github.com/user-attachments/assets/051d2eca-b4db-44e1-a8c9-09adaf810021)


### ğŸ“ Folder Structure

```arduino

â”œâ”€â”€ cli_app.py
â”œâ”€â”€ dag_langgraph.py
â”œâ”€â”€ fine_tune_sentiments.py  # (optional, for training)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ classification_log.jsonl
â”œâ”€â”€ fine_tuned_model/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ tokenizer/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  Credits

- Fine-tuned using the IMDb dataset  
- Backup via [`facebook/bart-large-mnli`](https://huggingface.co/facebook/bart-large-mnli)  
- Built with [Hugging Face Transformers](https://huggingface.co) and [LangGraph](https://python.langchain.com/docs/langgraph)  
- ğŸ’» Developed by **Adarsh Prasad**

---
